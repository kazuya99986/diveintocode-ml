{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled42.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21ceb6e6677549ad868803d62d171b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a09674414ae34750bbe9bb2222bcf9f7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_21264133374a458c85b9537c6ee1cc1a",
              "IPY_MODEL_f879a95989dc438dbd9ea6b508a475c8"
            ]
          }
        },
        "a09674414ae34750bbe9bb2222bcf9f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21264133374a458c85b9537c6ee1cc1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e5eb22623af641ebb011ca00a3be0b17",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3443f948ce80464691cf5d53d372ae15"
          }
        },
        "f879a95989dc438dbd9ea6b508a475c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f9ad2bd5a3224385a0b72c59e1ecf385",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:00&lt;00:00,  6.02 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_340431bc5067410e9554246cd3734d96"
          }
        },
        "e5eb22623af641ebb011ca00a3be0b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3443f948ce80464691cf5d53d372ae15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9ad2bd5a3224385a0b72c59e1ecf385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "340431bc5067410e9554246cd3734d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u01wEchOyr_K"
      },
      "source": [
        "# Sprint13 TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YEgw6dTv-t5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGP9NTcPJzdS"
      },
      "source": [
        "# 【問題1】スクラッチを振り返る\n",
        "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv7Rv82OJ0if"
      },
      "source": [
        "*   重みを初期化する必要があった\n",
        "*   エポックのループが必要だった\n",
        "*   バッチ処理\n",
        "*   バックプロパゲーション\n",
        "*   クロスエントロピー誤差による損失の計算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pM2Bgm8ygyf"
      },
      "source": [
        "# 【問題2】スクラッチとTensorFlowの対応を考える\n",
        "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\n",
        "\n",
        "\n",
        "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KbmE8qPyklx"
      },
      "source": [
        "＜回答＞\n",
        "*   重みを初期化する必要があった\n",
        "→シェイプを整え、ランダム値で設定\n",
        "*   エポックのループが必要だった\n",
        "→for文で記載\n",
        "*   バッチ処理\n",
        "→エポックの中のさらなるループで処理\n",
        "*   バックプロパゲーション\n",
        "→不明\n",
        "*   クロスエントロピー誤差による損失の計算\n",
        "→フォワードプロパゲーションの後で計算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlZwuOkaxSkz",
        "outputId": "26969a28-0863-4fe1-e91f-d9fb2ef64ee3"
      },
      "source": [
        "\"\"\"\n",
        "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 重みとバイアスの宣言\n",
        "        self.w1 = tf.Variable(tf.random.normal([n_input, n_hidden1]), trainable=True)\n",
        "        self.w2 = tf.Variable(tf.random.normal([n_hidden1, n_hidden2]), trainable=True)\n",
        "        self.w3 = tf.Variable(tf.random.normal([n_hidden2, n_classes]), trainable=True)\n",
        "        self.b1 = tf.Variable(tf.random.normal([n_hidden1]), trainable=True)\n",
        "        self.b2 = tf.Variable(tf.random.normal([n_hidden2]), trainable=True)\n",
        "        self.b3 = tf.Variable(tf.random.normal([n_classes]), trainable=True)\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        単純な3層ニューラルネットワーク\n",
        "        \"\"\"\n",
        "        layer_1 = tf.add(tf.matmul(x, self.w1), self.b1)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.add(tf.matmul(layer_1, self.w2), self.b2)\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        layer_output = tf.matmul(layer_2, self.w3) + self.b3  # tf.addと+は等価である\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "def train(x, y):\n",
        "    logits = model(x, training=True)\n",
        "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, logits))#2値分類特有\n",
        "    return loss\n",
        "\n",
        "def evaluate(x, y):\n",
        "    logits = model(x)\n",
        "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, logits)) #2値分類特有\n",
        "    # 推定結果\n",
        "    correct_pred = tf.equal(tf.sign(y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "    # 指標値計算\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "    return loss, accuracy\n",
        "\n",
        "################\n",
        "#メイン処理\n",
        "################\n",
        "\n",
        "# データセットの読み込み\n",
        "iris_dataset = load_iris()\n",
        "\n",
        "temp_X = iris_dataset.data\n",
        "temp_y = iris_dataset.target.reshape(-1,1)\n",
        "\n",
        "iris_marged = np.concatenate((temp_X, temp_y), axis=1)\n",
        "iris_marged = iris_marged[iris_marged[:,4] >= 1]\n",
        "\n",
        "iris_marged[:,4] = np.where(iris_marged[:,4] == 1, 0, 1)\n",
        "\n",
        "X = iris_marged[:, 0:4].astype(np.float32)\n",
        "y = iris_marged[:, 4]\n",
        "y = y.astype(np.float32).reshape(-1,1)\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "\n",
        "#インスタンス化\n",
        "model = MyModel()\n",
        "\n",
        "\n",
        "# # 最適化手法\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "for epoch in range(num_epochs):\n",
        "    # エポックごとにループ\n",
        "    total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "\n",
        "    for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "        # ミニバッチごとにループ\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = train(mini_batch_x, mini_batch_y)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        total_loss += loss\n",
        "\n",
        "    loss = total_loss / n_samples\n",
        "    val_loss, val_acc = evaluate(X_val, y_val)\n",
        "\n",
        "    print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, val_acc))\n",
        "\n",
        "_, test_acc = evaluate(X_test, y_test)\n",
        "print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 14.9768, val_loss : 93.8475, val_acc : 0.375\n",
            "Epoch 1, loss : 5.7823, val_loss : 15.9875, val_acc : 0.625\n",
            "Epoch 2, loss : 2.0068, val_loss : 8.2886, val_acc : 0.375\n",
            "Epoch 3, loss : 0.2590, val_loss : 4.1528, val_acc : 0.375\n",
            "Epoch 4, loss : 0.0938, val_loss : 0.8891, val_acc : 0.750\n",
            "Epoch 5, loss : 0.1017, val_loss : 0.0776, val_acc : 0.938\n",
            "Epoch 6, loss : 0.0653, val_loss : 2.2378, val_acc : 0.750\n",
            "Epoch 7, loss : 0.1921, val_loss : 4.7458, val_acc : 0.688\n",
            "Epoch 8, loss : 0.3616, val_loss : 4.4804, val_acc : 0.688\n",
            "Epoch 9, loss : 0.4438, val_loss : 2.2927, val_acc : 0.812\n",
            "test_acc : 0.850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlL-vspR7J3e"
      },
      "source": [
        "# 【問題3】3種類すべての目的変数を使用したIrisのモデルを作成\n",
        "Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類すべてを分類できるモデルを作成してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOjLtWD97RHD",
        "outputId": "fde0d8c5-34d1-44ce-f340-36f4715c3f08"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 重みとバイアスの宣言\n",
        "        self.w1 = tf.Variable(tf.random.normal([n_input, n_hidden1]), trainable=True)\n",
        "        self.w2 = tf.Variable(tf.random.normal([n_hidden1, n_hidden2]), trainable=True)\n",
        "        self.w3 = tf.Variable(tf.random.normal([n_hidden2, n_classes]), trainable=True)\n",
        "        self.b1 = tf.Variable(tf.random.normal([n_hidden1]), trainable=True)\n",
        "        self.b2 = tf.Variable(tf.random.normal([n_hidden2]), trainable=True)\n",
        "        self.b3 = tf.Variable(tf.random.normal([n_classes]), trainable=True)\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        単純な3層ニューラルネットワーク\n",
        "        \"\"\"\n",
        "        layer_1 = tf.add(tf.matmul(x, self.w1), self.b1)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.add(tf.matmul(layer_1, self.w2), self.b2)\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        layer_output = tf.matmul(layer_2, self.w3) + self.b3  # tf.addと+は等価である\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "def train(x, y):\n",
        "    logits = model(x, training=True)\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits, axis=1)) #多値分類の場合はsoftmax（確率算出）\n",
        "   \n",
        "    return loss\n",
        "\n",
        "def evaluate(x, y):\n",
        "    logits = model(x)\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits, axis=1))#多値分類の場合はsoftmax（確率算出）\n",
        "    \n",
        "    # 推定結果\n",
        "    correct_pred = tf.equal( tf.argmax(y, axis=1), tf.argmax(logits, axis=1) )#多値分類の場合はこう（accも）\n",
        "    # 指標値計算\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "    return loss, accuracy\n",
        "\n",
        "\n",
        "# データセットの読み込み\n",
        "iris_dataset = load_iris()\n",
        "\n",
        "X = iris_dataset.data.astype(np.float32)\n",
        "y = iris_dataset.target.astype(np.int32)\n",
        "y = np.identity(3)[y] #onehot\n",
        "\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 50\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "\n",
        "#インスタンス化\n",
        "model = MyModel()\n",
        "\n",
        "\n",
        "# # 最適化手法\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "for epoch in range(num_epochs):\n",
        "    # エポックごとにループ\n",
        "    total_batch = np.ceil(X_train.shape[0] / batch_size).astype(np.int)\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "\n",
        "    for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "        # ミニバッチごとにループ\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = train(mini_batch_x, mini_batch_y)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        total_loss += np.sum(loss)\n",
        "\n",
        "    loss = total_loss / n_samples\n",
        "    val_loss, val_acc = evaluate(X_val, y_val)\n",
        "\n",
        "    print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, val_acc))\n",
        "\n",
        "_, test_acc = evaluate(X_test, y_test)\n",
        "print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 6.8708, val_loss : 5.5243, val_acc : 0.708\n",
            "Epoch 1, loss : 0.7200, val_loss : 11.9320, val_acc : 0.708\n",
            "Epoch 2, loss : 0.2627, val_loss : 1.3556, val_acc : 0.833\n",
            "Epoch 3, loss : 0.0326, val_loss : 1.3494, val_acc : 0.833\n",
            "Epoch 4, loss : 0.0347, val_loss : 1.8996, val_acc : 0.917\n",
            "Epoch 5, loss : 0.0181, val_loss : 1.2068, val_acc : 0.958\n",
            "Epoch 6, loss : 0.0193, val_loss : 1.2095, val_acc : 0.958\n",
            "Epoch 7, loss : 0.0171, val_loss : 1.4245, val_acc : 0.958\n",
            "Epoch 8, loss : 0.0150, val_loss : 1.7277, val_acc : 0.917\n",
            "Epoch 9, loss : 0.0130, val_loss : 1.0894, val_acc : 0.958\n",
            "Epoch 10, loss : 0.0135, val_loss : 1.0986, val_acc : 0.958\n",
            "Epoch 11, loss : 0.0126, val_loss : 1.3210, val_acc : 0.958\n",
            "Epoch 12, loss : 0.0129, val_loss : 2.1461, val_acc : 0.917\n",
            "Epoch 13, loss : 0.0199, val_loss : 1.3692, val_acc : 0.833\n",
            "Epoch 14, loss : 0.0821, val_loss : 2.1141, val_acc : 0.833\n",
            "Epoch 15, loss : 0.0644, val_loss : 1.4303, val_acc : 0.958\n",
            "Epoch 16, loss : 0.0118, val_loss : 4.0251, val_acc : 0.917\n",
            "Epoch 17, loss : 0.0805, val_loss : 3.0024, val_acc : 0.792\n",
            "Epoch 18, loss : 0.1321, val_loss : 2.8408, val_acc : 0.833\n",
            "Epoch 19, loss : 0.0869, val_loss : 1.1734, val_acc : 0.833\n",
            "Epoch 20, loss : 0.0343, val_loss : 1.9888, val_acc : 0.958\n",
            "Epoch 21, loss : 0.0358, val_loss : 4.6157, val_acc : 0.875\n",
            "Epoch 22, loss : 0.0847, val_loss : 5.6033, val_acc : 0.833\n",
            "Epoch 23, loss : 0.0648, val_loss : 4.7478, val_acc : 0.875\n",
            "Epoch 24, loss : 0.0947, val_loss : 2.2725, val_acc : 0.875\n",
            "Epoch 25, loss : 0.1390, val_loss : 1.5864, val_acc : 0.833\n",
            "Epoch 26, loss : 0.0651, val_loss : 1.3056, val_acc : 0.958\n",
            "Epoch 27, loss : 0.0028, val_loss : 2.0630, val_acc : 0.958\n",
            "Epoch 28, loss : 0.0609, val_loss : 5.2656, val_acc : 0.875\n",
            "Epoch 29, loss : 0.0557, val_loss : 0.9066, val_acc : 0.917\n",
            "Epoch 30, loss : 0.0715, val_loss : 2.4405, val_acc : 0.875\n",
            "Epoch 31, loss : 0.0978, val_loss : 0.7673, val_acc : 0.917\n",
            "Epoch 32, loss : 0.0345, val_loss : 1.8331, val_acc : 0.958\n",
            "Epoch 33, loss : 0.0485, val_loss : 5.7480, val_acc : 0.833\n",
            "Epoch 34, loss : 0.1499, val_loss : 3.4960, val_acc : 0.917\n",
            "Epoch 35, loss : 0.0766, val_loss : 3.2409, val_acc : 0.792\n",
            "Epoch 36, loss : 0.1198, val_loss : 0.8603, val_acc : 0.917\n",
            "Epoch 37, loss : 0.0829, val_loss : 3.1857, val_acc : 0.917\n",
            "Epoch 38, loss : 0.0411, val_loss : 5.7110, val_acc : 0.875\n",
            "Epoch 39, loss : 0.1541, val_loss : 2.0001, val_acc : 0.917\n",
            "Epoch 40, loss : 0.0246, val_loss : 3.9654, val_acc : 0.750\n",
            "Epoch 41, loss : 0.0991, val_loss : 1.8303, val_acc : 0.833\n",
            "Epoch 42, loss : 0.0618, val_loss : 3.3748, val_acc : 0.917\n",
            "Epoch 43, loss : 0.0350, val_loss : 1.7795, val_acc : 0.958\n",
            "Epoch 44, loss : 0.0152, val_loss : 3.4490, val_acc : 0.917\n",
            "Epoch 45, loss : 0.0212, val_loss : 1.4676, val_acc : 0.833\n",
            "Epoch 46, loss : 0.0386, val_loss : 1.9343, val_acc : 0.917\n",
            "Epoch 47, loss : 0.0042, val_loss : 4.7900, val_acc : 0.917\n",
            "Epoch 48, loss : 0.0603, val_loss : 5.6911, val_acc : 0.750\n",
            "Epoch 49, loss : 0.1783, val_loss : 2.8365, val_acc : 0.833\n",
            "test_acc : 0.967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md4nenqUw5x5"
      },
      "source": [
        "# 【問題4】House Pricesのモデルを作成\n",
        "回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。\n",
        "\n",
        "\n",
        "House Prices: Advanced Regression Techniques\n",
        "\n",
        "\n",
        "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使ってください。説明変数はさらに増やしても構いません。\n",
        "\n",
        "\n",
        "分類問題と回帰問題の違いを考慮してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KKFxJ4Awr_T",
        "outputId": "7d1c7767-d2f1-4c1a-cf86-62c6260fe598"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 重みとバイアスの宣言\n",
        "        self.w1 = tf.Variable(tf.random.normal([n_input, n_hidden1]), trainable=True)\n",
        "        self.w2 = tf.Variable(tf.random.normal([n_hidden1, n_hidden2]), trainable=True)\n",
        "        self.w3 = tf.Variable(tf.random.normal([n_hidden2, n_classes]), trainable=True)\n",
        "        self.b1 = tf.Variable(tf.random.normal([n_hidden1]), trainable=True)\n",
        "        self.b2 = tf.Variable(tf.random.normal([n_hidden2]), trainable=True)\n",
        "        self.b3 = tf.Variable(tf.random.normal([n_classes]), trainable=True)\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        単純な3層ニューラルネットワーク\n",
        "        \"\"\"\n",
        "        layer_1 = tf.add(tf.matmul(x, self.w1), self.b1)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.add(tf.matmul(layer_1, self.w2), self.b2)\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        layer_output = tf.matmul(layer_2, self.w3) + self.b3  # tf.addと+は等価である\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "def train(x, y):\n",
        "    logits = model(x, training=True)\n",
        "    loss = tf.keras.metrics.mean_squared_error(y, logits)\n",
        "   \n",
        "    return loss\n",
        "\n",
        "def evaluate(x, y):\n",
        "    logits = model(x)\n",
        "    loss = tf.keras.metrics.mean_squared_error(y, logits)\n",
        "    \n",
        "    # 推定結果\n",
        "    mae = tf.keras.metrics.mean_absolute_error(y, logits)\n",
        "\n",
        "    return loss, mae\n",
        "\n",
        "\n",
        "# データセットの読み込み\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv('drive/My Drive/data/house-prices-advanced-regression-techniques_train.csv')\n",
        "X = df[['GrLivArea', 'YearBuilt']].values\n",
        "y = df['SalePrice'].values.reshape(-1,1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "\n",
        "#インスタンス化\n",
        "model = MyModel()\n",
        "\n",
        "\n",
        "# # 最適化手法\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "for epoch in range(num_epochs):\n",
        "    # エポックごとにループ\n",
        "    total_batch = np.ceil(X_train.shape[0] / batch_size).astype(np.int)\n",
        "    total_loss = 0\n",
        "    val_mae_total = 0\n",
        "\n",
        "    for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "        # ミニバッチごとにループ\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = train(mini_batch_x, mini_batch_y)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        total_loss += np.mean(loss)\n",
        "\n",
        "    loss = total_loss\n",
        "    val_loss, val_mae = evaluate(X_val, y_val)\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_mae_total += np.mean(val_mae)\n",
        "\n",
        "    print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, val_mae : {:.3f}\".format(epoch, loss, val_loss, val_mae_total))\n",
        "\n",
        "_, test_mae = evaluate(X_test, y_test)\n",
        "test_mae = np.mean(test_mae)\n",
        "print(\"test_mae : {:.3f}\".format(test_mae))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 388452843520.0000, val_loss : 37070831616.0000, val_mae : 177701.328\n",
            "Epoch 1, loss : 387762069504.0000, val_loss : 36991324160.0000, val_mae : 177474.562\n",
            "Epoch 2, loss : 386795079680.0000, val_loss : 36870176768.0000, val_mae : 177138.531\n",
            "Epoch 3, loss : 385304393728.0000, val_loss : 36681842688.0000, val_mae : 176624.578\n",
            "Epoch 4, loss : 383012239360.0000, val_loss : 36396298240.0000, val_mae : 175852.312\n",
            "Epoch 5, loss : 379580866560.0000, val_loss : 35976261632.0000, val_mae : 174720.484\n",
            "Epoch 6, loss : 374608404480.0000, val_loss : 35379175424.0000, val_mae : 173110.781\n",
            "Epoch 7, loss : 367650959360.0000, val_loss : 34558554112.0000, val_mae : 170890.844\n",
            "Epoch 8, loss : 358235131904.0000, val_loss : 33468020736.0000, val_mae : 167916.172\n",
            "Epoch 9, loss : 345907888128.0000, val_loss : 32067313664.0000, val_mae : 164044.062\n",
            "Epoch 10, loss : 330310803456.0000, val_loss : 30327226368.0000, val_mae : 159141.094\n",
            "Epoch 11, loss : 311211890688.0000, val_loss : 28234629120.0000, val_mae : 153086.516\n",
            "Epoch 12, loss : 288545961984.0000, val_loss : 25792528384.0000, val_mae : 145767.422\n",
            "Epoch 13, loss : 262458261504.0000, val_loss : 23035494400.0000, val_mae : 137122.250\n",
            "Epoch 14, loss : 233394151424.0000, val_loss : 20029786112.0000, val_mae : 127147.914\n",
            "Epoch 15, loss : 202142345216.0000, val_loss : 16875504640.0000, val_mae : 115864.344\n",
            "Epoch 16, loss : 169853348864.0000, val_loss : 13708292096.0000, val_mae : 103365.383\n",
            "Epoch 17, loss : 137990223872.0000, val_loss : 10689775616.0000, val_mae : 89973.781\n",
            "Epoch 18, loss : 108165904384.0000, val_loss : 7984250880.0000, val_mae : 76201.109\n",
            "Epoch 19, loss : 81958440448.0000, val_loss : 5732875264.0000, val_mae : 62854.316\n",
            "Epoch 20, loss : 60619810560.0000, val_loss : 4026032384.0000, val_mae : 50583.547\n",
            "Epoch 21, loss : 44783643136.0000, val_loss : 2873392896.0000, val_mae : 41120.652\n",
            "Epoch 22, loss : 34239054592.0000, val_loss : 2200908288.0000, val_mae : 35297.914\n",
            "Epoch 23, loss : 28050008448.0000, val_loss : 1874761728.0000, val_mae : 32147.230\n",
            "Epoch 24, loss : 24864666752.0000, val_loss : 1750247040.0000, val_mae : 31076.154\n",
            "Epoch 25, loss : 23391449088.0000, val_loss : 1715411200.0000, val_mae : 31020.023\n",
            "Epoch 26, loss : 22715863040.0000, val_loss : 1706976384.0000, val_mae : 31113.176\n",
            "Epoch 27, loss : 22349384448.0000, val_loss : 1699576192.0000, val_mae : 31105.326\n",
            "Epoch 28, loss : 22090011648.0000, val_loss : 1688242688.0000, val_mae : 31009.027\n",
            "Epoch 29, loss : 21874794496.0000, val_loss : 1674537728.0000, val_mae : 30861.676\n",
            "Epoch 30, loss : 21686744704.0000, val_loss : 1660183936.0000, val_mae : 30690.754\n",
            "Epoch 31, loss : 21517592064.0000, val_loss : 1646717568.0000, val_mae : 30519.461\n",
            "Epoch 32, loss : 21363290752.0000, val_loss : 1634447360.0000, val_mae : 30360.252\n",
            "Epoch 33, loss : 21220749952.0000, val_loss : 1623377920.0000, val_mae : 30228.766\n",
            "Epoch 34, loss : 21087205120.0000, val_loss : 1613248512.0000, val_mae : 30109.555\n",
            "Epoch 35, loss : 20961975552.0000, val_loss : 1603942400.0000, val_mae : 29999.350\n",
            "Epoch 36, loss : 20844242944.0000, val_loss : 1595157760.0000, val_mae : 29890.902\n",
            "Epoch 37, loss : 20733039360.0000, val_loss : 1587046528.0000, val_mae : 29787.381\n",
            "Epoch 38, loss : 20629669632.0000, val_loss : 1579303936.0000, val_mae : 29685.576\n",
            "Epoch 39, loss : 20532428672.0000, val_loss : 1572135552.0000, val_mae : 29592.471\n",
            "Epoch 40, loss : 20440522240.0000, val_loss : 1565582592.0000, val_mae : 29506.494\n",
            "Epoch 41, loss : 20353061888.0000, val_loss : 1559277312.0000, val_mae : 29417.816\n",
            "Epoch 42, loss : 20269955840.0000, val_loss : 1553275136.0000, val_mae : 29330.299\n",
            "Epoch 43, loss : 20192541568.0000, val_loss : 1547751296.0000, val_mae : 29247.766\n",
            "Epoch 44, loss : 20119252736.0000, val_loss : 1542444032.0000, val_mae : 29166.572\n",
            "Epoch 45, loss : 20050702336.0000, val_loss : 1537087104.0000, val_mae : 29082.607\n",
            "Epoch 46, loss : 19985262720.0000, val_loss : 1531969024.0000, val_mae : 29001.365\n",
            "Epoch 47, loss : 19924190080.0000, val_loss : 1527143424.0000, val_mae : 28922.951\n",
            "Epoch 48, loss : 19866093952.0000, val_loss : 1522542976.0000, val_mae : 28846.846\n",
            "Epoch 49, loss : 19810628224.0000, val_loss : 1518165888.0000, val_mae : 28773.820\n",
            "Epoch 50, loss : 19757721600.0000, val_loss : 1513707392.0000, val_mae : 28697.820\n",
            "Epoch 51, loss : 19706842368.0000, val_loss : 1509445504.0000, val_mae : 28627.115\n",
            "Epoch 52, loss : 19659283392.0000, val_loss : 1505511168.0000, val_mae : 28563.615\n",
            "Epoch 53, loss : 19614427456.0000, val_loss : 1501781760.0000, val_mae : 28507.451\n",
            "Epoch 54, loss : 19570828544.0000, val_loss : 1498157696.0000, val_mae : 28451.945\n",
            "Epoch 55, loss : 19528964672.0000, val_loss : 1494685952.0000, val_mae : 28397.436\n",
            "Epoch 56, loss : 19488685824.0000, val_loss : 1491344256.0000, val_mae : 28342.811\n",
            "Epoch 57, loss : 19449448576.0000, val_loss : 1487974016.0000, val_mae : 28289.949\n",
            "Epoch 58, loss : 19412321856.0000, val_loss : 1484730880.0000, val_mae : 28238.650\n",
            "Epoch 59, loss : 19376183936.0000, val_loss : 1481701888.0000, val_mae : 28188.502\n",
            "Epoch 60, loss : 19341511360.0000, val_loss : 1478919680.0000, val_mae : 28143.586\n",
            "Epoch 61, loss : 19308876032.0000, val_loss : 1476083200.0000, val_mae : 28093.150\n",
            "Epoch 62, loss : 19277115392.0000, val_loss : 1473324160.0000, val_mae : 28035.295\n",
            "Epoch 63, loss : 19249216192.0000, val_loss : 1470756864.0000, val_mae : 27984.676\n",
            "Epoch 64, loss : 19221249920.0000, val_loss : 1468338048.0000, val_mae : 27940.961\n",
            "Epoch 65, loss : 19194753024.0000, val_loss : 1466025216.0000, val_mae : 27897.834\n",
            "Epoch 66, loss : 19168903104.0000, val_loss : 1463786368.0000, val_mae : 27854.863\n",
            "Epoch 67, loss : 19144269312.0000, val_loss : 1461648256.0000, val_mae : 27816.508\n",
            "Epoch 68, loss : 19118937984.0000, val_loss : 1459633024.0000, val_mae : 27782.773\n",
            "Epoch 69, loss : 19095078720.0000, val_loss : 1457699840.0000, val_mae : 27754.820\n",
            "Epoch 70, loss : 19071318336.0000, val_loss : 1455676928.0000, val_mae : 27722.908\n",
            "Epoch 71, loss : 19048700480.0000, val_loss : 1453629056.0000, val_mae : 27691.180\n",
            "Epoch 72, loss : 19026580416.0000, val_loss : 1451715968.0000, val_mae : 27663.146\n",
            "Epoch 73, loss : 19006787648.0000, val_loss : 1449866624.0000, val_mae : 27638.914\n",
            "Epoch 74, loss : 18986155520.0000, val_loss : 1447836672.0000, val_mae : 27608.576\n",
            "Epoch 75, loss : 18965996544.0000, val_loss : 1445937152.0000, val_mae : 27579.297\n",
            "Epoch 76, loss : 18947400832.0000, val_loss : 1444332160.0000, val_mae : 27557.555\n",
            "Epoch 77, loss : 18929000448.0000, val_loss : 1442839680.0000, val_mae : 27533.836\n",
            "Epoch 78, loss : 18911036800.0000, val_loss : 1441435392.0000, val_mae : 27510.812\n",
            "Epoch 79, loss : 18893145344.0000, val_loss : 1440013824.0000, val_mae : 27487.457\n",
            "Epoch 80, loss : 18876489408.0000, val_loss : 1438670336.0000, val_mae : 27470.820\n",
            "Epoch 81, loss : 18860237568.0000, val_loss : 1437334144.0000, val_mae : 27454.205\n",
            "Epoch 82, loss : 18843663104.0000, val_loss : 1435938304.0000, val_mae : 27432.371\n",
            "Epoch 83, loss : 18828224320.0000, val_loss : 1434612864.0000, val_mae : 27415.082\n",
            "Epoch 84, loss : 18813256064.0000, val_loss : 1433393152.0000, val_mae : 27399.781\n",
            "Epoch 85, loss : 18797671424.0000, val_loss : 1432164352.0000, val_mae : 27381.734\n",
            "Epoch 86, loss : 18782439488.0000, val_loss : 1430906240.0000, val_mae : 27365.219\n",
            "Epoch 87, loss : 18766897216.0000, val_loss : 1429767424.0000, val_mae : 27351.025\n",
            "Epoch 88, loss : 18752098432.0000, val_loss : 1428721152.0000, val_mae : 27337.678\n",
            "Epoch 89, loss : 18737928320.0000, val_loss : 1427696384.0000, val_mae : 27324.094\n",
            "Epoch 90, loss : 18723411136.0000, val_loss : 1426711168.0000, val_mae : 27307.719\n",
            "Epoch 91, loss : 18709182912.0000, val_loss : 1425903744.0000, val_mae : 27296.684\n",
            "Epoch 92, loss : 18695773632.0000, val_loss : 1425218688.0000, val_mae : 27290.252\n",
            "Epoch 93, loss : 18682607616.0000, val_loss : 1424435584.0000, val_mae : 27276.445\n",
            "Epoch 94, loss : 18670525504.0000, val_loss : 1423714432.0000, val_mae : 27265.695\n",
            "Epoch 95, loss : 18659749760.0000, val_loss : 1423028352.0000, val_mae : 27254.539\n",
            "Epoch 96, loss : 18648278464.0000, val_loss : 1422403456.0000, val_mae : 27243.607\n",
            "Epoch 97, loss : 18637551232.0000, val_loss : 1421827840.0000, val_mae : 27235.701\n",
            "Epoch 98, loss : 18627520768.0000, val_loss : 1421341184.0000, val_mae : 27231.164\n",
            "Epoch 99, loss : 18616251904.0000, val_loss : 1420812672.0000, val_mae : 27227.523\n",
            "test_mae : 30361.350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7tCnPbCy0PW"
      },
      "source": [
        "# 【問題5】MNISTのモデルを作成\n",
        "ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。\n",
        "\n",
        "\n",
        "3クラス以上の分類という点ではひとつ前のIrisと同様です。入力が画像であるという点で異なります。\n",
        "\n",
        "\n",
        "スクラッチで実装したモデルの再現を目指してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAQVbSac6sxa",
        "outputId": "e6ff8a79-2b4b-423c-e2ae-21cf29bacc47"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 重みとバイアスの宣言\n",
        "        self.w1 = tf.Variable(tf.random.normal([n_input, n_hidden1]), trainable=True)\n",
        "        self.w2 = tf.Variable(tf.random.normal([n_hidden1, n_hidden2]), trainable=True)\n",
        "        self.w3 = tf.Variable(tf.random.normal([n_hidden2, n_classes]), trainable=True)\n",
        "        self.b1 = tf.Variable(tf.random.normal([n_hidden1]), trainable=True)\n",
        "        self.b2 = tf.Variable(tf.random.normal([n_hidden2]), trainable=True)\n",
        "        self.b3 = tf.Variable(tf.random.normal([n_classes]), trainable=True)\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        単純な3層ニューラルネットワーク\n",
        "        \"\"\"\n",
        "        layer_1 = tf.add(tf.matmul(x, self.w1), self.b1)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.add(tf.matmul(layer_1, self.w2), self.b2)\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        layer_output = tf.matmul(layer_2, self.w3) + self.b3  # tf.addと+は等価である\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "def train(x, y):\n",
        "    logits = model(x, training=True)\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits, axis=1)) #多値分類の場合はsoftmax（確率算出）\n",
        "   \n",
        "    return loss\n",
        "\n",
        "def evaluate(x, y):\n",
        "    logits = model(x)\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits, axis=1))#多値分類の場合はsoftmax（確率算出）\n",
        "    \n",
        "    # 推定結果\n",
        "    correct_pred = tf.equal( tf.argmax(y, axis=1), tf.argmax(logits, axis=1) )#多値分類の場合はこう（accも）\n",
        "    # 指標値計算\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "    return loss, accuracy\n",
        "\n",
        "\n",
        "# データセットの読み込み\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784).astype(np.float32)\n",
        "X_test = X_test.reshape(-1, 784).astype(np.float32)\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "\n",
        "\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 5\n",
        "n_hidden1 = 400\n",
        "n_hidden2 = 200\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "\n",
        "#インスタンス化\n",
        "model = MyModel()\n",
        "\n",
        "\n",
        "# # 最適化手法\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "for epoch in range(num_epochs):\n",
        "    # エポックごとにループ\n",
        "    total_batch = np.ceil(X_train.shape[0] / batch_size).astype(np.int)\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "\n",
        "    for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "        # ミニバッチごとにループ\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = train(mini_batch_x, mini_batch_y)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        total_loss += np.sum(loss)\n",
        "\n",
        "    loss = total_loss / n_samples\n",
        "    val_loss, val_acc = evaluate(X_val, y_val)\n",
        "\n",
        "    print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, val_acc))\n",
        "\n",
        "_, test_acc = evaluate(X_test, y_test_one_hot)\n",
        "print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 124.3265, val_loss : 3192.2466, val_acc : 0.914\n",
            "Epoch 1, loss : 16.8057, val_loss : 1806.1128, val_acc : 0.925\n",
            "Epoch 2, loss : 7.8272, val_loss : 1396.7643, val_acc : 0.929\n",
            "Epoch 3, loss : 4.9731, val_loss : 1111.8914, val_acc : 0.938\n",
            "Epoch 4, loss : 3.3195, val_loss : 1040.7947, val_acc : 0.938\n",
            "test_acc : 0.937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OScVV-8U6vmb"
      },
      "source": [
        "モジュール版"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198,
          "referenced_widgets": [
            "21ceb6e6677549ad868803d62d171b77",
            "a09674414ae34750bbe9bb2222bcf9f7",
            "21264133374a458c85b9537c6ee1cc1a",
            "f879a95989dc438dbd9ea6b508a475c8",
            "e5eb22623af641ebb011ca00a3be0b17",
            "3443f948ce80464691cf5d53d372ae15",
            "f9ad2bd5a3224385a0b72c59e1ecf385",
            "340431bc5067410e9554246cd3734d96"
          ]
        },
        "id": "2ZxOaYEowErd",
        "outputId": "a7397cf2-db31-4fa6-ae65-0f2ec0c11b1b"
      },
      "source": [
        "#\n",
        "#データの準備\n",
        "#\n",
        "\n",
        "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True) #データ読み込み\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test'] #トレインテストスプリット\n",
        "\n",
        "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples #トレインから10%のバリデーション作成\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "#0〜255を0と1の間に縮める（演算スピードが上がる、計算精度が上がる）\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255.\n",
        "  return image, label\n",
        "\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "buffer_size = 10000 #メモリに気を使い、ちょっとずつ処理を進める\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(buffer_size) #シャッフル\n",
        "\n",
        "#トレインとバリデーションの作成\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples) #skip=引数のデータを除く\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "#トレインのみバッチを回す（テストとバリデーションはフォワードのみのため不要）\n",
        "train_data = train_data.batch(batch_size) \n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21ceb6e6677549ad868803d62d171b77",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68fbVcSxyOxa"
      },
      "source": [
        "#モデルの作成\n",
        "\n",
        "input_size = 784 #28*28\n",
        "output_size = 10\n",
        "hidden_layer_size = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FlG1Ug531Tq"
      },
      "source": [
        "#最適化アルゴリズムと損失関数の決定\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4nZTX1q4Wp4",
        "outputId": "044f3103-3282-40d5-88cb-4c000115974e"
      },
      "source": [
        "#学習\n",
        "num_epochs = 5\n",
        "validation_steps = num_validation_samples #バッチサイズ（バリデーションはバッチに分けず一回で実施のため）\n",
        "\n",
        "model.fit(train_data, epochs=num_epochs, validation_data=(validation_inputs, validation_targets), validation_steps=validation_steps, verbose=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "540/540 - 13s - loss: 0.4106 - accuracy: 0.8843 - val_loss: 0.2050 - val_accuracy: 0.9413\n",
            "Epoch 2/5\n",
            "540/540 - 8s - loss: 0.1897 - accuracy: 0.9456 - val_loss: 0.1656 - val_accuracy: 0.9492\n",
            "Epoch 3/5\n",
            "540/540 - 8s - loss: 0.1431 - accuracy: 0.9576 - val_loss: 0.1340 - val_accuracy: 0.9587\n",
            "Epoch 4/5\n",
            "540/540 - 9s - loss: 0.1175 - accuracy: 0.9646 - val_loss: 0.1072 - val_accuracy: 0.9678\n",
            "Epoch 5/5\n",
            "540/540 - 9s - loss: 0.0994 - accuracy: 0.9705 - val_loss: 0.0969 - val_accuracy: 0.9710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f900a872350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI3qar2D5tKj",
        "outputId": "8992d9cd-ee64-41df-e299-166c6e0aa137"
      },
      "source": [
        "#推定\n",
        "test_loss, test_accuracy = model.evaluate(test_data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step - loss: 0.1111 - accuracy: 0.9679\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}