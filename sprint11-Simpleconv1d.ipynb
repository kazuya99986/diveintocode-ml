{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled46.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCactLMnyJ4U"
      },
      "source": [
        "# Sprint11　Simpleconv1d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBU_Z_aayJ7C"
      },
      "source": [
        "# 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
        "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
        "\n",
        "\n",
        "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A70FRFMbL1hS"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Scratch1dCNNClassifier:\n",
        "    '''\n",
        "    NN畳み込み層\n",
        "    '''\n",
        "    def __init__(self, W, B, stride=1, pad=0):\n",
        "        '''\n",
        "        初期化\n",
        "        ---\n",
        "        引数\n",
        "\n",
        "        W: 重み\n",
        "        B: バイアス\n",
        "        stride: ストライド数 \n",
        "        pad: パディング数 #パディングとは特徴マップの縁を0で埋める処理\n",
        "        ---\n",
        "        '''\n",
        "        self.W = W\n",
        "        self.B = B\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "\n",
        "        if W.ndim == 1:\n",
        "            FH = 1\n",
        "        else:\n",
        "            FH = W.shape[1] #フィルターの高さ\n",
        "        FW = self.W.shape[0] #フィルターの幅\n",
        "\n",
        "        if X.ndim == 1:\n",
        "            XH = 1\n",
        "        else:\n",
        "            XH = X.shape[1] #Xの高さ\n",
        "        XW = X.shape[0] #Xの幅\n",
        "\n",
        "        F = W.size\n",
        "\n",
        "        #畳み込み演算\n",
        "        self.out = np.array([])\n",
        "        for i in range(FH):\n",
        "            for j in range(FW - self.stride):\n",
        "                self.out = np.append(self.out, np.dot(X[j : FW+j ], self.W.T) + self.B)\n",
        "\n",
        "        return self.out\n",
        "\n",
        "\n",
        "X = np.array([1,2,3,4])\n",
        "W = np.array([3, 5, 7])\n",
        "B = np.array([1])\n",
        "\n",
        "a = Scratch1dCNNClassifier(W, B)\n",
        "out = a.forward(X)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl3Nfwn3yJ96"
      },
      "source": [
        "# 【問題2】1次元畳み込み後の出力サイズの計算\n",
        "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiSyToqZ4TYK",
        "outputId": "94644f76-8ede-4fb2-8d8e-97579f1dba56"
      },
      "source": [
        "def Conv_output_cal(N_in, pad, F, stride):\n",
        "    return (N_in + 2 * pad - F) / stride + 1\n",
        "\n",
        "N_in = X.shape[0]\n",
        "pad = 1\n",
        "F = W.shape[0]\n",
        "stride = 1\n",
        "\n",
        "Conv_output_cal(N_in, pad, F, stride)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Y-uTjcyKAS"
      },
      "source": [
        "# 【問題3】小さな配列での1次元畳み込み層の実験\n",
        "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3Oos4a0PQTq",
        "outputId": "a8268af4-f185-4fbc-8ea5-179c669ef733"
      },
      "source": [
        "class Scratch1dCNNClassifier:\n",
        "    '''\n",
        "    NN畳み込み層\n",
        "    '''\n",
        "    def __init__(self, W, B, lr, stride, pad):\n",
        "        '''\n",
        "        初期化\n",
        "        ---\n",
        "        引数\n",
        "\n",
        "        W: 重み\n",
        "        B: バイアス\n",
        "        lr = 学習率\n",
        "        stride: ストライド数 \n",
        "        pad: パディング数 #パディングとは特徴マップの縁を0で埋める処理\n",
        "        ---\n",
        "        '''\n",
        "        self.W = W\n",
        "        self.B = B\n",
        "        self.lr = lr\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        # 中間データ（backward時に使用）\n",
        "        self.X = None   \n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "        \n",
        "        # 重み・バイアスパラメータの勾配\n",
        "        self.dW = []\n",
        "        self.db = None\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "\n",
        "        if W.ndim == 1:\n",
        "            FH = 1\n",
        "        else:\n",
        "            FH = W.shape[1] #フィルターの高さ\n",
        "        FW = self.W.shape[0] #フィルターの幅\n",
        "\n",
        "        if X.ndim == 1:\n",
        "            XH = 1\n",
        "        else:\n",
        "            XH = X.shape[1] #Xの高さ\n",
        "        XW = X.shape[0] #Xの幅\n",
        "\n",
        "\n",
        "        #畳み込み演算\n",
        "        self.out = np.array([])\n",
        "        for i in range(FH):\n",
        "            for j in range(FW - self.stride):\n",
        "                self.out = np.append(self.out, np.dot(X[j : FW+j ], self.W.T) + self.B)\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def backward(self , dA):\n",
        "        X_index = np.arange(dA.shape[0]) #インデックス値\n",
        "        n_iter = int(self.X.shape[0] - dA.shape[0] // self.stride + 1)\n",
        "        \n",
        "        #バイアス勾配計算\n",
        "        self.dB = np.sum(dA)\n",
        "\n",
        "        #重み勾配計算\n",
        "        for k in range(n_iter):\n",
        "            self.dW.append( np.dot(self.X[X_index + k].T, dA) )\n",
        "\n",
        "        #出力値計算\n",
        "        dZ = np.zeros(self.X.shape[0])\n",
        "\n",
        "        for i in range(dA.shape[0]):\n",
        "            dZ[i : i + self.W.shape[0] ] += self.W * dA[i]\n",
        "\n",
        "        #重み、バイアス更新\n",
        "        self.dW = np.array(self.dW)\n",
        "        self.W = self.W - self.lr * self.dW\n",
        "        self.B = self.B - self.lr * self.dW\n",
        "        \n",
        "        return dZ\n",
        "\n",
        "\n",
        "X = np.array([1,2,3,4])\n",
        "W = np.array([3, 5, 7])\n",
        "B = np.array([1])\n",
        "lr = 0.01\n",
        "stride = 1\n",
        "pad = 0\n",
        "\n",
        "b = Scratch1dCNNClassifier(W, B, lr, stride, pad).forward(X)\n",
        "dA = np.array([10, 20]) #流れてきた誤差\n",
        "dZ = b.backward(dA)\n",
        "\n",
        "print(dZ)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 30. 110. 170. 140.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4bisrgeyKDQ"
      },
      "source": [
        "# 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
        "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A-r-QXEVHxg",
        "outputId": "b656bc55-903c-4865-b986-a7f73d97e15e"
      },
      "source": [
        "class Conv1d:\n",
        "    '''\n",
        "    NN畳み込み層\n",
        "    '''\n",
        "    def __init__(self, W, B, lr, stride, pad):\n",
        "        '''\n",
        "        初期化\n",
        "        ---\n",
        "        引数\n",
        "\n",
        "        W: 重み\n",
        "        B: バイアス\n",
        "        lr = 学習率\n",
        "        stride: ストライド数 \n",
        "        pad: パディング数 #パディングとは特徴マップの縁を0で埋める処理\n",
        "        ---\n",
        "        '''\n",
        "        self.W = W\n",
        "        self.B = B\n",
        "        self.lr = lr\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        # 中間データ（backward時に使用）\n",
        "        self.X = None   \n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "        \n",
        "        # 重み・バイアスパラメータの勾配\n",
        "        self.dW = []\n",
        "        self.db = None\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "\n",
        "        if self.W.ndim == 1:\n",
        "            FH = 1\n",
        "        else:\n",
        "            FH = 1 #フィルターの高さ\n",
        "        FW = self.W.shape[2] #フィルターの幅\n",
        "        C = self.W.shape[1] #フィルター出力チャンネル数\n",
        "        FN = self.W.shape[0] #フィルター入力チャンネル数\n",
        "\n",
        "        if X.ndim == 1:\n",
        "            XH = 1\n",
        "        else:\n",
        "            XH = 1\n",
        "        XH = 1 #Xの高さ\n",
        "        XW = X.shape[1] #Xの幅\n",
        "        XC = X.shape[0] #Xの入力チャンネル数\n",
        "\n",
        "\n",
        "        #畳み込み演算\n",
        "        self.out = []\n",
        "        for i in range(FN):\n",
        "            temp_out_2 = []\n",
        "            for k in range(C):\n",
        "                temp_out_1 = []\n",
        "                for j in range(FW - self.stride):\n",
        "                    temp_out_1.append( np.dot( X[k ,  j : FW + j ], self.W[i, k].T ) )\n",
        "                temp_out_2.append(sum(temp_out_1) + self.B[i])\n",
        "            self.out.append(temp_out_2)\n",
        "\n",
        "        return self.out\n",
        "\n",
        "\n",
        "X = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
        "W = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
        "B = np.array([1, 2, 3]) # （出力チャンネル数）\n",
        "lr = 0.01\n",
        "stride = 1\n",
        "pad = 0\n",
        "\n",
        "c = Conv1d(W, B, lr, stride, pad)\n",
        "c_out = c.forward(X)\n",
        "print(c_out)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[16.0, 22.0], [17.0, 23.0], [18.0, 24.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9g1UuYl968s"
      },
      "source": [
        "# 【問題8】学習と推定\n",
        "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
        "\n",
        "\n",
        "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
        "\n",
        "\n",
        "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R6GRdl7Jnfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da933b08-7b5d-4208-9c7a-7b5edc676dcf"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_val_one_hot = enc.transform(y_val[:, np.newaxis])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoAmqmvd99n7"
      },
      "source": [
        "class FC():\n",
        "    \"\"\"\n",
        "    ノード数n_nodes1からn_nodes2への全結合層\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      前の層のノード数\n",
        "    n_nodes2 : int\n",
        "      後の層のノード数\n",
        "    initializer : 初期化方法のインスタンス\n",
        "    optimizer : 最適化手法のインスタンス\n",
        "    \"\"\"\n",
        "    def __init__(self, input, output, initializer, optimizer, activation):\n",
        "        self.optimizer = optimizer\n",
        "        self.input = input\n",
        "        self.output = output\n",
        "        self.initializer = initializer\n",
        "\n",
        "         # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する       \n",
        "        self.W = self.initializer.W(self.input, self.output, activation)\n",
        "        self.B = self.initializer.B(self.output)\n",
        "        self.W_Adagrad = Adagrad()\n",
        "        self.B_Adagrad = Adagrad()\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        フォワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            入力\n",
        "        Returns\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            出力\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "\n",
        "        A = np.dot(X, self.W ) + self.B\n",
        "\n",
        "        return A\n",
        "\n",
        "\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        バックワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            後ろから流れてきた勾配\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            前に流す勾配\n",
        "        \"\"\"\n",
        "        \n",
        "        Z = np.dot(dA, self.W.T)\n",
        "        self.dA = dA\n",
        "\n",
        "        # 更新\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return Z"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD3oNbH9cxd4"
      },
      "source": [
        "class Convolutional:\n",
        "    '''\n",
        "    NN畳み込み層\n",
        "    '''\n",
        "    def __init__(self, filter_size, input_shape, stride, pad, initializer, optimizer, activation):\n",
        "        self.filter_size = filter_size\n",
        "        self.input_shape = input_shape\n",
        "        self.input_C = input_shape[0]\n",
        "        self.input_W = input_shape[1]\n",
        "        self.input_H = input_shape[2]\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer\n",
        "        self.activation = activation\n",
        "\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        self.FN = 3\n",
        "        self.FH = 1\n",
        "        self.FW = filter_size #1次元データの為\n",
        "        self.W = self.initializer.W(self.input_C, self.FW, activation)\n",
        "        self.B = self.initializer.B(self.input_C) #仮（本来チャネル数）\n",
        "        self.W_Adagrad = Adagrad()\n",
        "        self.B_Adagrad = Adagrad()\n",
        "\n",
        "        # 重み・バイアスパラメータの勾配\n",
        "        self.dW = []\n",
        "        self.db = None        \n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "\n",
        "        #畳み込み演算\n",
        "        # A = np.array([])\n",
        "        # for i in range(self.FH):\n",
        "        #     for j in range(self.FW - self.stride):\n",
        "        #         A = np.append(A, np.dot(X[ j : self.FW+j ].T, self.W.T) + self.B)\n",
        "\n",
        "        # for i in range(self.FN):\n",
        "        #     temp_out_2 = []\n",
        "\n",
        "        self.OW = int( 1 + (self.input_W + 2 * pad - self.FW) / self.stride ) #ループ回数\n",
        "        A = np.zeros((self.input_C, self.OW))\n",
        "        \n",
        "\n",
        "        for k in range(self.input_C):\n",
        "            temp_out_1 = []\n",
        "\n",
        "            for j in range(self.OW):\n",
        "                rows = np.sum(np.dot( X[k ,  j : self.FW + j ].reshape(16,1), self.W[k, :].reshape(1,16) ) )\n",
        "                temp_out_1.append(rows)\n",
        "\n",
        "            cols = temp_out_1 + self.B[:,k]\n",
        "            A[0] = cols\n",
        "\n",
        "            #アウトプット：768\n",
        "\n",
        "        return A\n",
        "\n",
        "\n",
        "    def backward(self , dA):\n",
        "        print(f'da   {dA.shape}')\n",
        "        self.dA = dA\n",
        "\n",
        "        X_index = np.arange(dA.shape[0]) #インデックス値\n",
        "        n_iter = int(self.X.shape[0] - dA.shape[0] // self.stride + 1)\n",
        "        \n",
        "        #バイアス勾配計算\n",
        "        self.dB = np.sum(dA, axis=1)\n",
        "\n",
        "        #重み勾配計算\n",
        "        for k in range(n_iter):\n",
        "            self.dW.append( np.dot(self.X[X_index + k].T, dA) )\n",
        "\n",
        "        #出力値計算\n",
        "        dZ = np.zeros_like(self.X, dtype='float64')\n",
        "\n",
        "        for k in range(self.input_C):\n",
        "            for j in range(self.OW - self.FW):\n",
        "                dZ[k , j : self.FW + j ] += self.W[k] * dA[k, j : self.FW + j ]\n",
        "\n",
        "        \n",
        "        # 更新\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "\n",
        "\n",
        "        # #重み、バイアス更新\n",
        "        # self.dW = np.array(self.dW)\n",
        "        # self.W = self.W - self.lr * self.dW\n",
        "        # self.B = self.B - self.lr * self.dW\n",
        "        \n",
        "        return dZ"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtAwvvk_A7m0"
      },
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_node1, n_node2, activation):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          前の層のノード数\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        Returns\n",
        "        ----------\n",
        "        W :\n",
        "            個別の重み\n",
        "        \"\"\"\n",
        "        if activation == 'Tanh' or 'sigmoid':\n",
        "            return self.sigma * XavierInitializer().cal(n_node1, n_node2)\n",
        "        else:\n",
        "            return self.sigma * HeInitializer().cal(n_node1, n_node2)\n",
        "\n",
        "    def B(self, n_node1):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        Returns\n",
        "        ----------\n",
        "        B :\n",
        "            個別のバイアス\n",
        "        \"\"\"\n",
        "        return np.random.randn(1, n_node1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30wDh3hgA--v"
      },
      "source": [
        "class SGD():\n",
        "    \"\"\"\n",
        "    確率的勾配降下法\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : 学習率\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        ある層の重みやバイアスの更新\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : 更新前の層のインスタンス\n",
        "        \"\"\"\n",
        "        #重み\n",
        "        print(layer.X.shape, layer.dA.shape)\n",
        "        W_grads = np.dot(layer.X.T, layer.dA) #XはAのこと\n",
        "        h = layer.W_Adagrad.update(layer.W, W_grads)\n",
        "        layer.W -= self.lr * W_grads / (np.sqrt(h[-1]) + 1e-7)\n",
        "\n",
        "        #バイアス\n",
        "        B_grads = np.sum(layer.dA, axis=0)\n",
        "        h = layer.B_Adagrad.update(layer.B, B_grads)\n",
        "        layer.B -= self.lr * B_grads / (np.sqrt(h[-1]) + 1e-7)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-jj_wv7BDaI"
      },
      "source": [
        "class Softmax:\n",
        "    '''\n",
        "    ソフトマックス関数の計算を行う\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
        "        return  self.Z\n",
        "\n",
        "    def backward(self, Y):\n",
        "        #ロス計算（クロスエントロピー）\n",
        "        delta = 1e-7\n",
        "        loss = -np.sum( Y * np.log(self.Z) + delta ) / len(Y)\n",
        "\n",
        "        return self.Z - Y,  loss\n",
        "\n",
        "\n",
        "class Tanh:\n",
        "    '''\n",
        "    ハイパボリックタンジェント関数の計算を行う\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        self.Z = np.tanh(A)\n",
        "        \n",
        "        return self.Z\n",
        "    \n",
        "    def backward(self, Z):\n",
        "        self.Z = Z * (1 - np.tanh(self.A) ** 2)\n",
        "        return self.Z"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNM_d3alBGUt"
      },
      "source": [
        "class ReLU:\n",
        "    '''\n",
        "    ReLu関数の計算を行う\n",
        "    '''\n",
        "    def __init__(self):# インスタンス変数　maskの初期化\n",
        "        self.mask = None\n",
        "        \n",
        "    def forward(self,A):#信号の大きさをxとして引数として渡す\n",
        "        self.mask = (A <=0 ) #  x <=0　の場合、True, それ以外はFalse を渡す\n",
        "        out = A.copy() #  xの値（配列）をコピーする\n",
        "        out[self.mask] = 0 # True の要素の値のみを０のに変換する\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, Z):\n",
        "        Z[self.mask] = 0 # x<=0がtrue のものは、逆伝播の微分のdoutも0で流す。それ以外はそのまま流す。\n",
        "        dx = Z\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFO-9i54BJDF"
      },
      "source": [
        "class XavierInitializer:\n",
        "    '''\n",
        "    シグモイド関数・ハイパボリックタンジェント関数を使用する際の重みの初期値\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def cal(self, node_input, node_output):\n",
        "        return np.random.randn(node_input, node_output) / np.sqrt(node_input)\n",
        "    \n",
        "    \n",
        "    \n",
        "class HeInitializer:\n",
        "    '''\n",
        "    ReLU関数を使用する際の重みの初期値\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        return\n",
        "\n",
        "    def cal(self, node_input, node_output):\n",
        "        return np.random.randn(node_input, node_output) * np.sqrt(1 / node_input)\n",
        "        \n",
        "        #/ np.sqrt(node_input) * np.sqrt(2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6rb3p2WBLkz"
      },
      "source": [
        "class Adagrad:\n",
        "    '''\n",
        "    学習率の最適化\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = np.zeros_like(params)\n",
        "\n",
        "        self.h += grads * grads\n",
        "\n",
        "        return self.h"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPiLe75EBTIk"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "6We5iVIPBXmA",
        "outputId": "de316b9b-a4c4-4416-93e8-85a39aa61f88"
      },
      "source": [
        "class ScratchDeepNeuralNetrowkClassifier():\n",
        "    '''\n",
        "    ベースのクラス\n",
        "    ---\n",
        "\n",
        "    '''\n",
        "    def __init__(self, filter_size1, filter_size2, stride, pad, input_shape, sigma, lr, n_epoch, batch_size, activation, n_node1, n_note2, n_output, verbose=False):\n",
        "        self.filter_size1 = filter_size1\n",
        "        self.filter_size2 = filter_size2\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        self.input_shape = input_shape\n",
        "        self.sigma = sigma\n",
        "        self.lr = lr\n",
        "        self.n_epoch = n_epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.activation = activation\n",
        "        self.n_node1 = n_node1\n",
        "        self.n_node2 = n_node2\n",
        "        self.n_output = n_output\n",
        "        self.verbose = verbose\n",
        "    \n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        '''\n",
        "        NN分類器を学習する\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            訓練データの特徴量\n",
        "        y : 次の形のndarray, shape (n_samples, )\n",
        "            訓練データの正解値\n",
        "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
        "            検証データの特徴量\n",
        "        y_val : 次の形のndarray, shape (n_samples, )\n",
        "            検証データの正解値\n",
        "        '''\n",
        "        self.n_features = input_shape[1] - self.filter_size1 + self.stride #全結合層接続時のinput数\n",
        "        self.loss_train = []\n",
        "        self.loss_val = []\n",
        "\n",
        "\n",
        "        optimizer = SGD(self.lr)\n",
        "\n",
        "        #畳み込み層インスタンス化\n",
        "        self.Conv1 = Convolutional(self.filter_size1, self.input_shape, self.stride, self.pad, SimpleInitializer(self.sigma), optimizer, self.activation)\n",
        "        self.activation1 = ReLU()\n",
        "\n",
        "        #全結合層インスタンス化\n",
        "        self.FC1 = FC(self.n_features, self.n_node1, SimpleInitializer(self.sigma), optimizer, self.activation)\n",
        "        self.activation2 = self.activation()\n",
        "        self.FC2 = FC(self.n_node1, self.n_node2, SimpleInitializer(self.sigma), optimizer, self.activation)\n",
        "        self.activation3 = self.activation()\n",
        "        self.FC3 = FC(self.n_node2, self.n_output, SimpleInitializer(self.sigma), optimizer, self.activation)\n",
        "        self.activation4 = Softmax()\n",
        "\n",
        "\n",
        "        \n",
        "        #エポックループ\n",
        "        for epoch in range(n_epoch):\n",
        "            self.forward_propagation(X)\n",
        "            self.backward_propagation(y)\n",
        "\n",
        "        self.forward_propagation(X)\n",
        "        dA4, loss = self.activation4.backward(y)\n",
        "        self.loss_train.append(loss)\n",
        "\n",
        "        if X_val is not None:\n",
        "            self.forward_propagation(X_val)\n",
        "            dA3, loss = self.activation3.backward(y_val)\n",
        "            self.loss_val.append(loss)\n",
        "\n",
        "\n",
        "        if self.verbose:\n",
        "            #verboseをTrueにした際は学習過程などを出力する\n",
        "            print()\n",
        "\n",
        "\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        A1 = self.Conv1.forward(X)\n",
        "        Z1 = self.activation1.forward(A1)\n",
        "        A2 = self.FC1.forward(Z1)\n",
        "        Z2 = self.activation2.forward(A2)\n",
        "        A3 = self.FC2.forward(Z2)\n",
        "        Z3 = self.activation3.forward(A3)\n",
        "        A4 = self.FC3.forward(Z3)\n",
        "        Z4 = self.activation4.forward(A4)\n",
        "        # print(vars(self.Conv1))\n",
        "\n",
        "\n",
        "\n",
        "    def backward_propagation(self, y):\n",
        "        dA4, _ = self.activation4.backward(y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
        "        dZ3 = self.FC3.backward(dA4)\n",
        "        dA3 = self.activation3.backward(dZ3)\n",
        "        dZ2 = self.FC2.backward(dA3)\n",
        "        dA2 = self.activation2.backward(dZ2)\n",
        "        dZ1 = self.FC1.backward(dA2)\n",
        "        dA1 = self.activation1.backward(dZ1)\n",
        "        dZ0 = self.Conv1.backward(dA1) # dZ0は使用しない\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        ニューラルネットワーク分類器を使い推定する。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            サンプル\n",
        "        Returns\n",
        "        -------\n",
        "        次の形のndarray, shape (n_samples, 1)\n",
        "            推定結果\n",
        "        '''\n",
        "\n",
        "        self.forward_propagation(X)\n",
        "        return np.argmax(self.activation3.__dict__['Z'], axis=1)\n",
        "\n",
        "\n",
        "sigma= 0.01\n",
        "lr = 0.01\n",
        "n_node1 = 400\n",
        "n_node2 = 200\n",
        "n_output = 10\n",
        "n_epoch = 1\n",
        "batch_size = 20\n",
        "activation = ReLU #Tanh or ReLU\n",
        "stride = 1\n",
        "pad = 0\n",
        "filter_size1 = 16\n",
        "filter_size2 = 32\n",
        "input_shape = [3, 784, 1]\n",
        "\n",
        "d = ScratchDeepNeuralNetrowkClassifier(filter_size1, filter_size2, stride, pad, input_shape, sigma, lr, n_epoch, batch_size, activation, n_node1, n_node2, n_output)\n",
        "d.fit(X_train[:3], y_train_one_hot[:3], X_val[:3], y_val_one_hot[:3])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 200) (3, 10)\n",
            "(3, 400) (3, 200)\n",
            "(3, 769) (3, 400)\n",
            "da   (3, 769)\n",
            "(3, 784) (3, 769)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4d83dbcc1128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScratchDeepNeuralNetrowkClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_size1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_node1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_node2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-4d83dbcc1128>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-4d83dbcc1128>\u001b[0m in \u001b[0;36mbackward_propagation\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mdA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mdZ0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dZ0は使用しない\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-7ee588a3355c>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dA)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# 更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-eb5eaf181000>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mW_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#XはAのこと\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_Adagrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW_grads\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-b9f0a127e451>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, params, grads)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,16) (784,769) (3,16) "
          ]
        }
      ]
    }
  ]
}