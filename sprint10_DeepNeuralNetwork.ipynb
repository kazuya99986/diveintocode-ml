{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sprint10-DeepNeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZw4B8Q47PdK"
      },
      "source": [
        "# Sprint10 DeepNeuralNetwork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_gWQqtoCrJA"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GHiVX3C7Pgn"
      },
      "source": [
        "# 【問題1】全結合層のクラス化\n",
        "全結合層のクラス化を行なってください。\n",
        "\n",
        "\n",
        "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
        "\n",
        "\n",
        "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
        "\n",
        "\n",
        "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、すべて全結合層が持つインスタンス変数にすることができます。\n",
        "\n",
        "\n",
        "初期化方法と最適化手法のクラスについては後述します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwFi-6h3CGKu"
      },
      "source": [
        "class FC():\n",
        "    \"\"\"\n",
        "    ノード数n_nodes1からn_nodes2への全結合層\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      前の層のノード数\n",
        "    n_nodes2 : int\n",
        "      後の層のノード数\n",
        "    initializer : 初期化方法のインスタンス\n",
        "    optimizer : 最適化手法のインスタンス\n",
        "    \"\"\"\n",
        "    def __init__(self, input, output, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.input = input\n",
        "        self.output = output\n",
        "        self.initializer = initializer\n",
        "\n",
        "         # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する       \n",
        "        self.W = self.initializer.W(self.input, self.output)\n",
        "        self.B = self.initializer.B(self.output)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        フォワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            入力\n",
        "        Returns\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            出力\n",
        "        \"\"\"\n",
        "        return np.dot(X, self.W ) + self.B\n",
        "\n",
        "\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        バックワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            後ろから流れてきた勾配\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            前に流す勾配\n",
        "        \"\"\"\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        # 更新\n",
        "        self = self.optimizer.update(self, dZ)\n",
        "        return dZ"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzY4v-9n7PjJ"
      },
      "source": [
        "# 【問題2】初期化方法のクラス化\n",
        "初期化を行うコードをクラス化してください。\n",
        "\n",
        "\n",
        "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
        "\n",
        "\n",
        "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFT9OOLOQ7Jw"
      },
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2, activation='tanh'):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          前の層のノード数\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        Returns\n",
        "        ----------\n",
        "        W :\n",
        "            個別の重み\n",
        "        \"\"\"\n",
        "        if activation == 'tanh' or 'sigmoid':\n",
        "            return self.sigma * XavierInitializer().cal(n_nodes1, n_nodes2)\n",
        "        else:\n",
        "            return self.sigma * HeInitializer().cal(n_nodes1, n_nodes2)\n",
        "\n",
        "    def B(self, n_nodes1, activation='tanh'):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        Returns\n",
        "        ----------\n",
        "        B :\n",
        "            個別のバイアス\n",
        "        \"\"\"\n",
        "        if activation == 'tanh' or 'sigmoid':\n",
        "            return np.random.randn(1, n_nodes1) #temp\n",
        "        else:\n",
        "            return np.random.randn(1,n_nodes1) #temp"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ9kgUZa7PmG"
      },
      "source": [
        "# 【問題3】最適化手法のクラス化\n",
        "最適化手法のクラス化を行なってください。\n",
        "\n",
        "\n",
        "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
        "\n",
        "\n",
        "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy8J9kTQaZhD"
      },
      "source": [
        "class SGD():\n",
        "    \"\"\"\n",
        "    確率的勾配降下法\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : 学習率\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer, dZ):\n",
        "        \"\"\"\n",
        "        ある層の重みやバイアスの更新\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : 更新前の層のインスタンス\n",
        "        \"\"\"\n",
        "        print(vars(layer))\n",
        "        layer.W -= self.lr * np.dot(dZ.T, layer.dA)  #back_W3 = np.dot(self.z2.T, back_a3)\n",
        "        layer.B -= self.lr * np.sum(layer.A, axis=0) # back_b3 = np.sum(back_a3, axis=0)\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQz8TWr07Poq"
      },
      "source": [
        "# 【問題4】活性化関数のクラス化\n",
        "活性化関数のクラス化を行なってください。\n",
        "\n",
        "\n",
        "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wvWTvhZdDjS"
      },
      "source": [
        "class Softmax:\n",
        "    '''\n",
        "    ソフトマックス関数の計算を行う\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def forward(self, A):\n",
        "        return np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1) \n",
        "\n",
        "    def backward(self, dZ, Y):\n",
        "        return dZ - Y\n",
        "\n",
        "\n",
        "class Tanh:\n",
        "    '''\n",
        "    ハイパボリックタンジェント関数の計算を行う\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.A = np.tanh(A)\n",
        "        return self.A\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - np.tanh(self.A) ** 2)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJuUYgft7PrQ"
      },
      "source": [
        "# 【問題5】ReLUクラスの作成\n",
        "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
        "\n",
        "\n",
        "ReLUは以下の数式です。\n",
        "\n",
        "$$\n",
        "f\n",
        "(\n",
        "x\n",
        ")\n",
        "=\n",
        "R\n",
        "e\n",
        "L\n",
        "U\n",
        "(\n",
        "x\n",
        ")\n",
        "=\n",
        "{\n",
        "x\n",
        "if \n",
        "x\n",
        ">\n",
        "0\n",
        ",\n",
        "0\n",
        "if \n",
        "x\n",
        "≦\n",
        "0\n",
        ".\n",
        "$$\n",
        "\n",
        "$x$ : ある特徴量。スカラー\n",
        "\n",
        "\n",
        "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
        "\n",
        "\n",
        "numpy.maximum — NumPy v1.15 Manual\n",
        "\n",
        "\n",
        "一方、バックプロパゲーションのための $x$ に関する $f(x)$ の微分は以下のようになります。\n",
        "\n",
        "$$\n",
        "∂\n",
        "f\n",
        "(\n",
        "x\n",
        ")\n",
        "∂\n",
        "x\n",
        "=\n",
        "{\n",
        "1\n",
        "if \n",
        "x\n",
        ">\n",
        "0\n",
        ",\n",
        "0\n",
        "if \n",
        "x\n",
        "≦\n",
        "0\n",
        ".\n",
        "$$\n",
        "\n",
        "数学的には微分可能ではないですが、 $x=0$ のとき $0$ とすることで対応しています。\n",
        "\n",
        "\n",
        "フォワード時の $x$ の正負により、勾配を逆伝播するかどうかが決まるということになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKZlQGHSa2Hv"
      },
      "source": [
        "class Relu():\n",
        "    '''\n",
        "    ReLu関数の計算を行う\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def cal(self, X):\n",
        "        return np.maximum(0, X)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goXjXRLh7PuF"
      },
      "source": [
        "# 【問題6】重みの初期値\n",
        "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
        "\n",
        "\n",
        "XavierInitializerクラスと、HeInitializerクラスを作成してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZi4g4KMrxZ2"
      },
      "source": [
        "class XavierInitializer:\n",
        "    '''\n",
        "    シグモイド関数・ハイパボリックタンジェント関数を使用する際の重みの初期値\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def cal(self, node_input, node_output):\n",
        "        return np.random.randn(node_input, node_output) / np.sqrt(node_input)\n",
        "    \n",
        "    \n",
        "    \n",
        "class HeInitializer:\n",
        "    '''\n",
        "    ReLU関数を使用する際の重みの初期値\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def cal(self, node_input, node_output):\n",
        "        return np.random.randn(node_input, node_output) / np.sqrt(node_input) * np.sqrt(2)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvTNlNnY7PvO"
      },
      "source": [
        "# 【問題7】最適化手法\n",
        "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
        "\n",
        "\n",
        "まず、これまで使ってきたSGDを確認します。\n",
        "\n",
        "$$\n",
        "W\n",
        "′\n",
        "i\n",
        "=\n",
        "W\n",
        "i\n",
        "−\n",
        "α\n",
        "E\n",
        "(\n",
        "∂\n",
        "L\n",
        "∂\n",
        "W\n",
        "i\n",
        ")\n",
        "B\n",
        "′\n",
        "i\n",
        "=\n",
        "B\n",
        "i\n",
        "−\n",
        "α\n",
        "E\n",
        "(\n",
        "∂\n",
        "L\n",
        "∂\n",
        "B\n",
        "i\n",
        ")\n",
        "$$\n",
        "\n",
        "$\\alpha$ : 学習率（層ごとに変えることも可能だが、基本的にはすべて同じとする）\n",
        "\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_i}$ : $W_i$ に関する損失 $L$ の勾配\n",
        "\n",
        "\n",
        "$\\frac{\\partial L}{\\partial B_i}$ : $B_i$ に関する損失 $L$ の勾配\n",
        "\n",
        "\n",
        "$E()$ : ミニバッチ方向にベクトルの平均を計算\n",
        "\n",
        "\n",
        "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
        "\n",
        "\n",
        "更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和 $H$ を保存しておき、その分だけ学習率を小さくします。\n",
        "\n",
        "\n",
        "学習率は重み一つひとつに対して異なることになります。\n",
        "\n",
        "$$\n",
        "H\n",
        "′\n",
        "i\n",
        "=\n",
        "H\n",
        "i\n",
        "+\n",
        "E\n",
        "(\n",
        "∂\n",
        "L\n",
        "∂\n",
        "W\n",
        "i\n",
        ")\n",
        "×\n",
        "E\n",
        "(\n",
        "∂\n",
        "L\n",
        "∂\n",
        "W\n",
        "i\n",
        ")\n",
        "W\n",
        "′\n",
        "i\n",
        "=\n",
        "W\n",
        "i\n",
        "−\n",
        "α\n",
        "1\n",
        "√\n",
        "H\n",
        "′\n",
        "i\n",
        "E\n",
        "(\n",
        "∂\n",
        "L\n",
        "∂\n",
        "W\n",
        "i\n",
        ")\n",
        "$$\n",
        "\n",
        "$H_i$ : i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
        "\n",
        "\n",
        "$H_i^{\\prime}$ : 更新した $H_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfRPwsEo5E66"
      },
      "source": [
        "class Adagrad:\n",
        "    '''\n",
        "    学習率の最適化\n",
        "    '''\n",
        "    def __init__(self, node_num):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h ={}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "        \n",
        "        for key in params, key():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            parms[key] -= self.lr * grads[key] / (np.sqrt( self.h[key] ) + 1e-7 )\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8zFF74t7Px-"
      },
      "source": [
        "# 【問題8】クラスの完成\n",
        "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6GqFzs0SroS",
        "outputId": "d3732882-72ed-4a9a-f9e7-1dee69f85100"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_val[:, np.newaxis])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOS6AcoRH1r4"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "AC36aJN7CJpq",
        "outputId": "e31ccde4-f95b-40ce-eca9-730948484f4d"
      },
      "source": [
        "class ScratchDeepNeuralNetrowkClassifier():\n",
        "    '''\n",
        "    ベースのクラス\n",
        "    ---\n",
        "\n",
        "    '''\n",
        "    def __init__(self, sigma, lr, n_node1, n_node2, n_output, n_epoch, batch_size, verbose=False):\n",
        "        self.verbose = verbose\n",
        "        self.sigma = sigma\n",
        "        self.lr = lr\n",
        "        self.n_nodes1 = n_node1\n",
        "        self.n_nodes2 = n_node2\n",
        "        self.n_output = n_output\n",
        "        self.n_epoch = n_epoch\n",
        "        self.batch_size = batch_size\n",
        "    \n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        '''\n",
        "        NN分類器を学習する\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            訓練データの特徴量\n",
        "        y : 次の形のndarray, shape (n_samples, )\n",
        "            訓練データの正解値\n",
        "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
        "            検証データの特徴量\n",
        "        y_val : 次の形のndarray, shape (n_samples, )\n",
        "            検証データの正解値\n",
        "        '''\n",
        "        self.n_features = X.shape[1]\n",
        "\n",
        "\n",
        "        optimizer = SGD(self.lr)\n",
        "\n",
        "\n",
        "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
        "        self.activation1 = Tanh()\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
        "        self.activation2 = Tanh()\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
        "        self.activation3 = Softmax()\n",
        "\n",
        "        #エポックループ\n",
        "        for epoch in range(n_epoch):\n",
        "            #ミニバッチ取得\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size=batch_size)\n",
        "            #バッチループ\n",
        "            for mini_X_train, mini_y_train in get_mini_batch:\n",
        "                # print(vars(self.FC1))\n",
        "\n",
        "                A1 = self.FC1.forward(mini_X_train)\n",
        "                Z1 = self.activation1.forward(A1)\n",
        "                A2 = self.FC2.forward(Z1)\n",
        "                Z2 = self.activation2.forward(A2)\n",
        "                A3 = self.FC3.forward(Z2)\n",
        "                Z3 = self.activation3.forward(A3)\n",
        "                \n",
        "\n",
        "\n",
        "                dA3 = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
        "                dZ2 = self.FC3.backward(dA3)\n",
        "                dA2 = self.activation2.backward(dZ2) \n",
        "                dZ1 = self.FC2.backward(dA2)\n",
        "                dA1 = self.activation1.backward(dZ1)\n",
        "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sigma= 0.01\n",
        "lr = 0.01\n",
        "n_nodes1 = 400\n",
        "n_nodes2 = 200\n",
        "n_output = 10\n",
        "n_epoch = 1\n",
        "batch_size = 100\n",
        "\n",
        "a = ScratchDeepNeuralNetrowkClassifier(sigma, lr, n_nodes1, n_nodes2, n_output, n_epoch, batch_size)\n",
        "a.fit(X_train, y_train_one_hot, X_val, y_test_one_hot)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'optimizer': <__main__.SGD object at 0x7f77fed38c50>, 'input': 200, 'output': 10, 'initializer': <__main__.SimpleInitializer object at 0x7f77ff56cd50>, 'W': array([[ 1.49287142e-04, -8.22676102e-04, -5.58927587e-04, ...,\n",
            "         2.89543987e-04, -6.73889337e-04,  7.50171396e-04],\n",
            "       [ 1.02159692e-03, -5.00480662e-04, -1.03430373e-03, ...,\n",
            "        -4.72541705e-04, -9.32873357e-04,  1.09918454e-03],\n",
            "       [ 7.07604328e-05,  7.86412460e-04,  2.71168333e-04, ...,\n",
            "        -5.31464250e-04,  2.11600570e-04, -3.42415805e-04],\n",
            "       ...,\n",
            "       [ 7.76552320e-04, -9.39236085e-04,  2.97892722e-04, ...,\n",
            "         1.22111209e-03,  1.51876731e-03, -3.65515919e-04],\n",
            "       [-6.55648182e-04, -5.52554131e-04, -1.07037298e-04, ...,\n",
            "        -8.26049310e-05,  5.83472747e-04, -1.74217941e-04],\n",
            "       [-5.52890392e-04,  8.60227671e-05,  1.41740182e-03, ...,\n",
            "         5.30549846e-04, -1.40613215e-04,  8.11609471e-04]]), 'B': array([[ 0.3984179 , -0.48485058,  0.43063656, -1.23565076,  0.87290986,\n",
            "        -1.047894  , -0.06202682, -0.77308458,  0.05861343, -1.34214235]])}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-b5318dbf2194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScratchDeepNeuralNetrowkClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nodes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nodes2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-75-b5318dbf2194>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mdA3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_y_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 交差エントロピー誤差とソフトマックスを合わせている\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mdZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0mdA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mdZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-3f4e7e1a1f66>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dA)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# 更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-d88119f87fcd>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, layer, dZ)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \"\"\"\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#back_W3 = np.dot(self.z2.T, back_a3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# back_b3 = np.sum(back_a3, axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'FC' object has no attribute 'dA'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KflG1Y4x7P0j"
      },
      "source": [
        "# 【問題9】学習と推定\n",
        "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
      ]
    }
  ]
}